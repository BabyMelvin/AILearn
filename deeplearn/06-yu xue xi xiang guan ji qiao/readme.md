# 1.参数更新

神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻 找最优参数的问题，
解决这个问题的过程称为最优化（optimization）

方法有：

* 1.随机梯度下降法（stochastic gradient descent）， 简称 SGD。
* 2.Momentum,动量. v = a*v - η * w',  
* 3.AdaGrad（adaptive 适当的）,学习率衰减(learning rate decay):随着学习进行，学习率逐渐减小
  * h = h + W' dot W'
* 4.Adam,融合 Momentum和Ada,

# 2.权重初始值


抑制过拟合、提高泛化能力的技巧——权值衰减（weight decay）。

* 权值衰减就是一种以减小权重参数的值为目的进行学习 的方法。
* 通过减小权重参数的值来抑制过拟合的发生。 

如果想减小权重的值，一开始就将初始值设为较小的值才是正途。

偏向 0 和 1 的数据分布会造成反向传播中梯度的值不断变小，最 后消失。这个问题称为梯度消失（gradient vanishing）。

权重初始值非常重要。很多时候权重初始 值的设定关系到神经网络的学习能否成功。

# 3.Batch Normalization算法

Batch Normalization（下文简称Batch Norm）是 2015 年提出的方法。优点：

* 可以使学习快速进行（可以增大学习率）。 
* 不那么依赖初始值（对于初始值不用那么神经质）。 
* 抑制过拟合（降低 Dropout 等的必要性）

# 4. 正则化
机器学习的问题中，过拟合是一个很常见的问题。g

## 过拟合

过拟合的原因:

* 模型拥有大量参数、表现力强。
* 训练数据少。

## 权值衰减
权值衰减是一直以来经常被使用的一种抑制过拟合的方法。

L2 范数相当于各个元素的平方和.  

用数学式表示的话，假设有权重 W = (w1,w2, ...,wn)，则L2范数可用 计算 出来。
除了 L2 范数，还有 L1 范数、L ∞范数等。L1 范数是各个元 素的绝对值之和，相当于|w1| + |w2| + ...+ |wn|。
L∞范数也称为 Max 范数，相当于各个元素的绝对值中最大的那一个。

## dropout
Dropout 是一种在学习的过程中随机删除神经元的方法。
训练时，随机 选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递，

* 训练时，每传递一次数据，就会随机选择要删除的神经元。
* 测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出， 要乘上训练时的删除比例后再输出。

# 5.超惨参数的验证
超参数（hyper-parameter）

这里所说的超参数是指，比如各层的神经元数量、batch 大小、参 数更新时的学习率或权值衰减等。
如果这些超参数没有设置合适的值，模型 的性能就会很差。

## 验证数据

注意:不能使用测试数据评估超参数的性能。这一点非常重要，但也容易被忽视。

用于调整超参 数的数据，一般称为验证数据（validation data）

## 超参数的最优化

进行超参数的最优化时，逐渐缩小超参数的“好值”的存在范围非常重要。

* 所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选 出一个超参数（采样），用这个采样到的值进行识别精度的评估；
* 然后，多次 重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围。
* 通过重复这一操作，就可以逐渐确定超参数的合适范围。