# 神经网络的学习

“学习”是指从训练数据中 自动获取最优权重参数的过程

`损失函数`: 学习过程就是将这个函数最小化(利用函数斜率的梯度法)

深 度 学 习 有 时 也 称 为 端 到 端 机 器 学 习（end-to-end machine learning）。

* 神经网络可以将数据直接作为原始数据，进行“端对端” 的学习。

只对某个数据集过度拟合的状态称为过拟合（`over fitting`），不能预测其他数据集。

# 损失函数

神经网络以某个指标为线索寻找最优权重参数。

损失函数可以是任意函数，一般均方误差和交叉熵误差等

## 均方误差 mean squared error

E = 1/2 * sum(yk - tk)^2

* yk 是表示神经网络的输出
* tk 表示监督数据
* k 表示数据的维数

## 交叉熵方差 cross entropy error

E = - sum(tk * log yk)

# 数值微分

## 导数

利用微小的差分求导数的过程称为数值微分（numerical differentiation）。

而 基 于 数 学 式 的 推 导 求 导 数 的 过 程，则 用“解析 性”（analytic）一词，称为“解析性求解”或者“解析性求导”。

## 偏导数

# 梯度

梯度的性质：

* 1.梯度是一个向量，（f(x'), f(y')）
* 2.梯度的模就是方向导数的最大值，也就是梯度是改点变化最大的地方
* 3.梯度表示是各点处函数值减小最多的方向，无法保证梯度所指方向就是最小值方向或真正应该前进的方向

## 梯度法 gradient method
函数的极小值、最小值以及被称为鞍点（saddle point）的地方， 梯度为 0。

* 极小值是局部最小值，某个范围最小值
* 鞍点从某个方向看是极大值，从另一个方向看则是极小值的点
* 梯度法是寻找梯度为0的地方，**但是那个地方不一定就是最小值**（可能是极小值或鞍点）
  * “学习高原”：当函数很复杂且呈扁平状，学习可能会进入一个平坦地区，无法前进的停滞期

根据目的是寻找最小值还是最大值，梯度法的叫法有所不同。

* 寻找最小值的梯度法称为梯度下降法（gradient descent method)
* 寻找最大值的梯度法称为梯度上升法（gradient ascent method）
但 是通过反转损失函数的符号，求最小值的问题和求最大值的问题会 变成相同的问题，因此“下降”还是“上升”的差异本质上并不重要


不断地沿梯度方向前进， 逐渐减小函数值的过程就是`梯度法`（gradient method）

`学习率`:learning rate, x0 = x0 - η * x0'， 这个η成为学习率.

* 学习率，决定一次学习中，应该学习多少，以及多大程度更新参数
* 学习率，过大或过小都无法抵达好位置。神经网络学习中，一般改变学习率，一边确认是否正确进行。

## 神经网络的梯度

神经网路学习也要求梯度。这里梯度是指损失函数关于权重的梯度。

这个方法通过梯度下降法更新 参数，不过因为这里使用的数据是随机选择的 mini batch 数据，
所以又称为 **随机梯度下降法**（stochastic gradient descent）。

“随机”指的是“随机选择的” 的意思，因此，随机梯度下降法是“对随机选择的数据进行的梯度下降法”。 深度学习的很多框架中，随机梯度下降法一般由一个名为 SGD 的函数来实现。 
SGD 来源于随机梯度下降法的英文名称的首字母。

## 过拟合问题
过拟合是指，虽然训练数据中的数字图像能 被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象。

定期训练数据和测试数据记录识别精度。 每经过一个epoch，记录训练数据和测试数据的识别精度.

`epoch`是一个单位。表示学习中**所有训练数据**均被使用过一次时的更新次数。
比如：10000笔数据集，用大小100笔数据mini-batch进行学习，重复随机梯度下降法100次，则所有数据都被“看”过，100次就是一个epoch