# 神经网络模型
神经网络模型,模型分为多个层，如：

第 0 层对应输入层，第 1 层对应中间层，第 2 层对应输出层

中间层有时也叫做隐藏层。

- 激活函数：激活函数是连接两层的函数。
- 输出层函数：输出层函数是决定输出值的函数，特殊的激活函数（用作输出层）。

神经网络可以用在分类问题和回归问题
- 分类问题：分类问题是根据某个输入预测一个（离散的） 类别， 如softmax函数。
- 回归问题：回归问题是根据某个输入预测一个（连续的） 数值的问题， 如恒等函数。

# 激活函数

神经网络使用的`激活函数`：

激活函数（activation function）:h（x）函数会将输入信号的总和转换为输出信号
* 阶跃函数:式（ 3.3）表示的激活函数以阈值为界，一旦输入超过阈值，就切换输出。 输出为0或1.
* sigmoid 函数： h(x) = 1 / (1 + exp(-x))
* ReLU（Rectified Linear Unit）函数:ReLU 函数在输入大于0 时，直接输出该值；在输入小于等于0 时，输 出 0

神经网络的激活函数必须使用非线性函数。激活函数不能使 用线性函数。
为什么不能使用线性函数呢？因为使用线性函数的话，加深神 经网络的层数就没有意义了。

线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无 隐藏层的神经网络”。
为了具体地（稍微直观地）理解这一点，我们来思 考下面这个简单的例子。这里我们考虑把线性函数h(x) = cx 
作为激活 函 数，把 y(x) = h(h(h(x))) 的运算对应3 层神经网络A。这个运算会进行
y(x) = c × c × c × x 的乘法运算，但是同样的处理可以由
y(x) = ax（注 意 ， a = c^3）这一次乘法运算（即没有隐藏层的神经网络）来表示。如本例所示，
使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所 带来的优势，激活函数必须使用非线性函数。

# 神经网络的内积

`Y = np.dot(X,W)`
- x输入层，W权重层，Y输出层


# 3层神经网络实现

神经网络前向处理：

forward() 函数中则封 装了将输入信号转换为输出信号的处理过程。
* 从输入到输出方向 的传递处理

我们将介绍后向（backward， 从`输出`到`输入`方向）的处理

# 输出层函数设计

神经网络可以用在分类问题和回归问题上。

回归问题用恒等函数，分类问题用 softmax 函数。

机器学习的问题大致可以分为分类问题和回归问题。

* 分类问题是数 据属于哪一个类别的问题。
* 而回归问题是根据某个输入预测一个（连续的） 数值的问题。

softmax 函数：`y(k) = exp(ak)/sum(exp(ai))`

注意：溢出问题，对策，减去输入信号的最大值,`exp_a = np.exp(a - c)`,见softmax函数实现。

输出层的神经元数量需要根据待解决的问题来决定。
对于分类问题，输 出层的神经元数量一般设定为类别的数量。


# MNIST

- `正规化`:normalization,将数据限定在某个范围处理，预处理一种。
- `预处理`：预处理在神经网络（深度学习）中非常实用，其有效性已在提高识别 性能和学习的效率等众多实验中得到证明。
很多预处理都会考虑到数据的整体分布：比如，利用数据 整体的均值或标准差，移动数据，使数据整体以 0 为中心分布，
或者进行正规化，把数据的延展控制在一定范围内。
除此之外，还有 将数据整体的分布形状均匀化的方法，即数据白化（whitening）等。
- `批处理`：batch。通过以批为单位进行推理处理，能够实现 高速的运算。