## 12.8 简单的并行编程
执行CPU密集型工作，想让它利用多核CPU优势来运行快一点。`concurrent.futures`提供一个`ProcessPoolExecutor`类，可被用来在一个独立Python解释器中执行计算密集型函数。下面一个脚本，这些日志文件中查找出所有访问robots.txt文件的主机:

```python
# findrobots.py
import gzip
import io
import glob

def find_robots(filenam):
	'''
	find all of the hosts that access robots.txt in single log file
	'''
	robots=set()
	with gzip.open(filename) as f:
		for line in io.TextTOWrapper(f,encoding='ascii')
			fields=line.split()
			if fileds[6]=='/robots.txt':
				robots.add(fields[0])
	return robots

def find_all_robots(logdir):
	'''
	find all hosts acroos and entire sequence of files
	'''
	files=glob.glob(logdir+'*.log.gz')
	all_robots=set()
	for robots in map(find_robots,files):
		all_robots.update(robots)
	return all_robots

if __name__ == `__main__`:
	robots=find_all_robots('log')
	for ipaddr in robots:
		print(ipaddr)
```
前面使用map-reduce风格来编写。函数`find_robots()`在一个文件名集合做map操作，并将结果汇总一个单独的结果，也就是find_all_robots()函数中all_robots集合。假设想用修改这个程序使用多核CPU。只需将map()替换成current.futures库中生成的类似操作即可。

```python
# findrobots.py
import gzip
import io
import glob

def find_robots(filenam):
	'''
	find all of the hosts that access robots.txt in single log file
	'''
	robots=set()
	with gzip.open(filename) as f:
		for line in io.TextTOWrapper(f,encoding='ascii')
			fields=line.split()
			if fileds[6]=='/robots.txt':
				robots.add(fields[0])
	return robots

def find_all_robots(logdir):
	'''
	find all hosts acroos and entire sequence of files
	'''
	files=glob.glob(logdir+'*.log.gz')
	all_robots=set()
	with futures.ProcessPoolExecutor() as pool:
		for robots in map(find_robots,files):
			all_robots.update(robots)
	return all_robots

if __name__ == `__main__`:
	robots=find_all_robots('log')
	for ipaddr in robots:
		print(ipaddr)
```
性能根据CPU数量不同而不同。四核比之前的快3.5倍。

**原理是**：一个ProcessPoolExecutor创建N个独立的Python解释器，N是体统上面可用CPU个数。可以通过可选参数给`ProcessPoolExecutor(N)`来修改处理器数量。处理池会一直运行到with块中最后一个语句执行完成，然后处理池被关闭。不过，程序会一直等待到所有提交的工作被处理完成。

被提交到池中工作必须定义为一个函数。有两种方法提交。如果想让一个列表推导或一个map()操作并执行的话，可使用`pool.map()`

```python
# a function that perform a lot of work
def work(x):
	...
	return result

# nonparallel code
results= map(work,data)

# parallel implementation
with ProcessPoolExecutor() as pool:
	retults=pool.map(work,data)
```
另外，可以使用pool.submit()来手动提交单个任务:

```python
# some function
def work(x):
	...
	return result

with ProcessPoolExecutor() as pool:
	...
	# example of submitting work to the pool
	future_result=pool.submit(work,arg)

	# obtaining the result (block until done)
	r=future_result.result()
```
如果你手动提交一个任务，结果是一个Future实例。要最终结果，需要调用它的`result()`方法，它会阻塞进程直到结果被返回来。

如果不想阻塞，还可以使用一个回调函数：
```python
def when_done(r):
	print('got',r.result())

with ProcessPoolExecutor() as pool:
	future_result=pool.submit(work,arg)
	future_result.add_done_callback(when_done)
```

回调函数接受一个Future实例，被用来获取最终的结果(比如通过调用它的`result()`方法).尽管处理池很容易使用，在设计大程序的时候还是有很多需要注意的地方：

* 这些是适用于被分解为互相独立部分问题
* 被提交必须是简单函数形式。对方法，闭包和其他类型并行执行还不支持
* 函数参数返回值必须兼容pickle，因为要使用到进程间的通信，所有解释器之间的交换数据必须被序列化
* 被提交的任务函数不应保留状态或副作用。除了日志，一旦启动不能控制子进程任何行为，最好保持简单和纯洁-函数不要去修改环境
* Unix上进程池通过调用`fork()`系统调用被创建，它会克隆Python解释器，包括fork时所有程序状态。而在window上，克隆解释器时不会可能状态。实际fork操作会在第一次调用pool.mao()或pool.submit()后发生。
* 应该在创建任何线程之前创建并激活进程池(比如程序启动的main线程中创建进程池)

## 12.9 Python全局锁问题
全局解释锁GIL，可能影响到多线程程序的执行性能。


Python完全支持多线程编程，但是**解释器C语言实现部分在完全并行执行时并不是线程安全的**。实际上，解释器别一个全局解释锁保护着，它确保任何时候只有一个Python线程执行。**GIL最大问题**就是Python的多线程并不能利用多核CPU优势（如果多线程密集型程序只能在一个单CPU上运行）。

**注意**：GIL只会影响到那些严重依赖CPU程序（比如计算型）。如果程序大部分只会涉及到I/O，比如网络交互，那么使用多线程就很合适，因为大部分时间都在等待。实际上，你完全可以放心的创建几千个Python线程，现代操作系统运行这么多线程没有任何压力，没啥可担心的。

对于依赖CPU程序，需要清楚执行的计算的特点。例如：优化底层算法要比使用多线程运行快得多。类似的，由于Pyhton是解释执行的，如果你将哪些性能瓶颈代码一直到一个C语言扩展模块中，速度也会提高的很快。Numpy操作数组很快，Pypy通过一个JIT解释器来优化执行效率（Python3?）

线程不是专门优化性能的。一个CPU依赖型程序可能会使用线程来管理一个图形用户界面，一个网络连接或其他服务。GIL会产生一些问题，如果一个长期持有GIL话会导致其他非CPU型线程一直等待。事实上一个写的不好的C语言扩展会导致这个问题更加严重，尽管代码计算部分会比之前运行的更快些。

我们有两种策略来解决GIL缺点。如果完全工作与Python 环境中，可以使用multiprocessing模块来创建一个进程池，并像协同处理器一样使用它。例如：加入如下线程代码：

```python
# performs a large calculation(CPU bound)
def some_work(args):
	...
	return return

# a thread that calls the above funtion
def some_thread():
	while True:
		...
		r=some_work(args)

#########修改为使用进程池###########
# processing pool(see below for initiazation)
pool =None

# performs a larget calculation (CPU bond)
def some_work(args):
	...
	return reuslt

# a thread that calls the above funcion
def some_thread():
	while True:
		...
		r=pool.apply(some_work,(args))
		...

# initiate the pool
if __name__ == '__main__':
	import multiprocessing
	pool=multiprocessing.Pool()
```
通过使用一个技巧利用进程池解决了GIL问题，当一个线程想要执行CPU密集型工作时，会将任务发给进程池。然后进程池启动一个单独的Python解释器来工作。当线程等待结果的时候被释放GLI。

另外一个解决GIL策略是使用C扩展编程技术。主要思想是将计算密集型任务转移给C，跟Python独立，在工作的时候在C代码中释放GIL。这可以通过在C代码中插入下面这样的特殊宏来完成:

```python
#include "Python.h"
...
PyObject* pyfunc(PyObject*self,PyObject * args){
	...
	Py_BEGIN_ALLOW_THREADS
	//Threaded C code
	...
	Py_END_ALLOW_THREADS
	...
}
```
如果你使用其他工具访问C语言如Cython的ctypes库，不需要做任何事，ctypes在调用C后会自动释放GIL。


